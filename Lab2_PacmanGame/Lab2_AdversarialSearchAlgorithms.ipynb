{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d252a8d",
   "metadata": {
    "id": "7d252a8d"
   },
   "source": [
    "***FCIM.FIA - Fundamentals of Artificial Intelligence***\n",
    "\n",
    "> **Lab 2:** *Adversarial Search Algorithms* \\\\\n",
    "> **Performed by:** *Bajenov Sevastian*, group *FAF-213* \\\\\n",
    "> **Verified by:** Elena Graur, asist. univ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BPiGwyyGNsHh",
   "metadata": {
    "id": "BPiGwyyGNsHh"
   },
   "source": [
    "## Imports and Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ccd62",
   "metadata": {},
   "source": [
    "Create a virtual environment, install all the necessary dependencies so that you can run the notebook using your virtual environment as a kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b3945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0059f9",
   "metadata": {},
   "source": [
    "In order to test Pacman's behavior use the following command:\n",
    "\n",
    "`python pacman.py -p AgentClassNameAgent -l mediumClassic -a depth=3 --frameTime 0`\n",
    "\n",
    "Where `AgentClassName` is the name of the class which inherits from the `MultiAgentSearchAgent` in the `multiAgents.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d864352",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "`Mini-max algorithm` is a recursive or backtracking algorithm which is used in decision-making and game theory. It provides an optimal move for the player assuming that opponent is also playing optimally. Mini-Max algorithm uses recursion to perform `adversarial` search through the game-tree. This Algorithm computes the minimax decision for the current state. It considers two players play the game, one is called MAX and other is called MIN. Both Players of the game are opponent of each other, where MAX will select the maximized value and MIN will select the minimized value.\n",
    "\n",
    "In order to implement `Mini-max algorithm` it is necessary to define an `evaluation function` which should define criteria of maximizing or minimizing the score. The provided formula in the conditions was `Score = Distance_to_nearest_pellet - Distance_to_nearest_ghost` and it's ovious that it is wrong because it reaches its maximum when the pellets are too far anf the ghosts are too close to Pacman. That is why I changed it a bit, by also considering current game score: `Score = GameScore + Distance_to_nearest_ghost - Distance_to_nearest_pellet`. Below is the implementation of the default minimax algorithm presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dfed673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run this cell\n",
    "\n",
    "class MinimaxAgent(MultiAgentSearchAgent):\n",
    "\n",
    "    def __init__(self, evalFn='scoreEvaluationFunction', depth='2'):\n",
    "        super().__init__(evalFn, depth)\n",
    "\n",
    "    def getAction(self, gameState):\n",
    "        bestAction = None\n",
    "        bestScore = float('-inf')\n",
    "\n",
    "        for action in gameState.getLegalActions(0):\n",
    "            score = self.minimax(\n",
    "                1, self.depth, gameState.generateSuccessor(0, action))\n",
    "\n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestAction = action\n",
    "\n",
    "        return bestAction\n",
    "\n",
    "    def minimax(self, agentIndex, depth, gameState):\n",
    "        if depth == 0 or gameState.isWin() or gameState.isLose():\n",
    "            score = self.evaluationFunction(gameState)\n",
    "            return score\n",
    "\n",
    "        numAgents = gameState.getNumAgents()\n",
    "        nextAgent = (agentIndex + 1) % numAgents\n",
    "        nextDepth = depth - 1 if nextAgent == 0 else depth\n",
    "\n",
    "        legalActions = gameState.getLegalActions(agentIndex)\n",
    "\n",
    "        if agentIndex == 0:\n",
    "            score = max(self.minimax(nextAgent, nextDepth, gameState.generateSuccessor(\n",
    "                agentIndex, action)) for action in legalActions)\n",
    "        else:\n",
    "            score = min(self.minimax(nextAgent, nextDepth, gameState.generateSuccessor(\n",
    "                agentIndex, action)) for action in legalActions)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d0c55",
   "metadata": {},
   "source": [
    "And here is the default evaluation function (manhattan_distance is being used as a heuristic parameter - distance between two points in the cartesian space):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run this cell\n",
    "\n",
    "def scoreEvaluationFunction(currentGameState):\n",
    "\n",
    "    def distanceToNearestPellet(gameState):\n",
    "        pacmanPos = gameState.getPacmanPosition()\n",
    "        food = gameState.getFood()\n",
    "        minDistance = float('inf')\n",
    "\n",
    "        for x in range(food.width):\n",
    "            for y in range(food.height):\n",
    "                if food[x][y]:\n",
    "                    distance = manhattanDistance(pacmanPos, (x, y))\n",
    "\n",
    "                    if distance < minDistance:\n",
    "                        minDistance = distance\n",
    "\n",
    "        return minDistance\n",
    "\n",
    "    def distanceToNearestGhost(gameState):\n",
    "        pacmanPos = gameState.getPacmanPosition()\n",
    "        ghostPositions = gameState.getGhostPositions()\n",
    "        minDistance = float('inf')\n",
    "\n",
    "        for ghostPos in ghostPositions:\n",
    "            distance = manhattanDistance(pacmanPos, ghostPos)\n",
    "\n",
    "            if distance < minDistance:\n",
    "                minDistance = distance\n",
    "\n",
    "        return minDistance\n",
    "\n",
    "    score = currentGameState.getScore()\n",
    "    score -= distanceToNearestPellet(currentGameState)\n",
    "    score += distanceToNearestGhost(currentGameState)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0684d26a",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "`Alpha-beta pruning` is a technique used to improve the efficiency of the minimax algorithm in game-playing AI. The technique is based on the observation that in many games, some branches of the game tree can be safely pruned (ignored) because they are guaranteed to be worse than other branches. \n",
    "\n",
    "The algorithm uses two parameters, `alpha` (best value for the maximizing player) and `beta` (best value for the minimizing player). The algorithm starts with alpha and beta having negative and positive infinity values, respectively. As the algorithm evaluates each node, it compares the score of the node with the current alpha and beta values. If the `current alpha value is > than the current beta value`, it means that the current branch is worse than a branch that has already been evaluated and can be safely pruned.\n",
    "\n",
    "Code implementation of the `Alpha-beta pruning` is rather starightforward and is presented further. It uses the same evaluation function as the `Mini-max` algorithm. It is important to mention that the Alpha-beta pruning enhances default Mini-max because less number of game states is being verified, thus speeding up Pacman decision making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51f61b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run this cell\n",
    "\n",
    "class AlphaBetaAgent(MultiAgentSearchAgent):\n",
    "\n",
    "    def getAction(self, gameState):\n",
    "        legalActions = gameState.getLegalActions(0)\n",
    "        bestAction = None\n",
    "        alpha = float('-inf')\n",
    "        beta = float('inf')\n",
    "        bestValue = float('-inf')\n",
    "\n",
    "        for action in legalActions:\n",
    "            value = self.alphaBeta(\n",
    "                1, self.depth, gameState.generateSuccessor(0, action), alpha, beta)\n",
    "\n",
    "            if value > bestValue:\n",
    "                bestValue = value\n",
    "                bestAction = action\n",
    "\n",
    "            alpha = max(alpha, bestValue)\n",
    "\n",
    "        return bestAction\n",
    "\n",
    "    def alphaBeta(self, agentIndex, depth, gameState, alpha, beta):\n",
    "        if depth == 0 or gameState.isWin() or gameState.isLose():\n",
    "            return self.evaluationFunction(gameState)\n",
    "\n",
    "        numAgents = gameState.getNumAgents()\n",
    "        nextAgent = (agentIndex + 1) % numAgents\n",
    "        nextDepth = depth - 1 if nextAgent == 0 else depth\n",
    "\n",
    "        legalActions = gameState.getLegalActions(agentIndex)\n",
    "\n",
    "        if agentIndex == 0:\n",
    "            value = float('-inf')\n",
    "\n",
    "            for action in legalActions:\n",
    "                value = max(value, self.alphaBeta(\n",
    "                    nextAgent, nextDepth, gameState.generateSuccessor(agentIndex, action), alpha, beta))\n",
    "                alpha = max(alpha, value)\n",
    "\n",
    "                if alpha >= beta:\n",
    "                    break\n",
    "\n",
    "            return value\n",
    "        else:\n",
    "            value = float('inf')\n",
    "\n",
    "            for action in legalActions:\n",
    "                value = min(value, self.alphaBeta(\n",
    "                    nextAgent, nextDepth, gameState.generateSuccessor(agentIndex, action), alpha, beta))\n",
    "                beta = min(beta, value)\n",
    "\n",
    "                if alpha >= beta:\n",
    "                    break\n",
    "\n",
    "            return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd47883",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "`Mini-max algorithm` is not ideal, especially the `evaluation function` which was provided before. Although it takes into consideration important parameters, many game states are still defined as equally-desirable by it (Pacman gets stuck and starts turning to the left and to the right without moving forward until a ghost appears). That is why I decided to modify evaluation function in several ways:\n",
    "\n",
    "* consider the number of pellets in the current region, their density within a certain radius;\n",
    "\n",
    "* make use of the ghost vulnerable state when Pacman eats an `ultra pellet` in its advantage;\n",
    "\n",
    "* analyze the complexity of the maze around Pacman so that it has to make less directional decisions;\n",
    "\n",
    "All of this parameters are being calculated within the score evaluation function by accessing current game state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8dfc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run this cell\n",
    "\n",
    "def scoreEvaluationFunction(currentGameState):\n",
    "\n",
    "    def distanceToNearestPellet(gameState):\n",
    "        pacmanPos = gameState.getPacmanPosition()\n",
    "        food = gameState.getFood()\n",
    "        minDistance = float('inf')\n",
    "\n",
    "        for x in range(food.width):\n",
    "            for y in range(food.height):\n",
    "                if food[x][y]:\n",
    "                    distance = manhattanDistance(pacmanPos, (x, y))\n",
    "\n",
    "                    if distance < minDistance:\n",
    "                        minDistance = distance\n",
    "\n",
    "        return minDistance\n",
    "\n",
    "    def distanceToNearestGhost(gameState):\n",
    "        pacmanPos = gameState.getPacmanPosition()\n",
    "        ghostPositions = gameState.getGhostPositions()\n",
    "        minDistance = float('inf')\n",
    "\n",
    "        for ghostPos in ghostPositions:\n",
    "            distance = manhattanDistance(pacmanPos, ghostPos)\n",
    "\n",
    "            if distance < minDistance:\n",
    "                minDistance = distance\n",
    "\n",
    "        return minDistance\n",
    "\n",
    "    def pelletNumberPerRegionParameter(gameState, regionSize=5):\n",
    "        pacmanPos = gameState.getPacmanPosition()\n",
    "        food = gameState.getFood()\n",
    "        walls = gameState.getWalls()\n",
    "        regionPelletCount = 0\n",
    "\n",
    "        for dx in range(-regionSize, regionSize + 1):\n",
    "            for dy in range(-regionSize, regionSize + 1):\n",
    "                x, y = pacmanPos[0] + dx, pacmanPos[1] + dy\n",
    "\n",
    "                if 0 <= x < walls.width and 0 <= y < walls.height and food[x][y]:\n",
    "                    regionPelletCount += 1\n",
    "\n",
    "        return regionPelletCount\n",
    "\n",
    "    def mazeComplexityParameter(gameState, radius=4):\n",
    "        pacmanPos = gameState.getPacmanPosition()\n",
    "        walls = gameState.getWalls()\n",
    "        complexity = 0\n",
    "\n",
    "        for dx in range(-radius, radius + 1):\n",
    "            for dy in range(-radius, radius + 1):\n",
    "                if dx == 0 and dy == 0:\n",
    "                    continue\n",
    "\n",
    "                x, y = pacmanPos[0] + dx, pacmanPos[1] + dy\n",
    "\n",
    "                if 0 <= x < walls.width and 0 <= y < walls.height and walls[x][y]:\n",
    "                    complexity += 1\n",
    "\n",
    "        return complexity\n",
    "\n",
    "    def ghostVulnerabilityParameter(gameState):\n",
    "        pacmanPos = gameState.getPacmanPosition()\n",
    "        scaredGhosts = [ghostState for ghostState in gameState.getGhostStates(\n",
    "        ) if ghostState.scaredTimer > 0]\n",
    "        minScaredGhostDistance = float('inf')\n",
    "\n",
    "        for ghostState in scaredGhosts:\n",
    "            ghostPos = ghostState.getPosition()\n",
    "            distance = manhattanDistance(pacmanPos, ghostPos)\n",
    "\n",
    "            if distance < minScaredGhostDistance:\n",
    "                minScaredGhostDistance = distance\n",
    "\n",
    "        return 1 / (minScaredGhostDistance + 1) if minScaredGhostDistance != float('inf') else 0\n",
    "\n",
    "    score = currentGameState.getScore()\n",
    "    score -= distanceToNearestPellet(currentGameState)\n",
    "    score += distanceToNearestGhost(currentGameState)\n",
    "    score += 0.5 * pelletNumberPerRegionParameter(currentGameState)\n",
    "    score += ghostVulnerabilityParameter(currentGameState)\n",
    "    score -= 0.25 * mazeComplexityParameter(currentGameState)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb62de2",
   "metadata": {},
   "source": [
    "It is worth noticing that the evaluation function is still not ideal. Although, Pacman doesn't get stuck when there are many pellets in the maze, but when there are two or three left it may avoid eating them because of the maze complexity. However, the results are much better than they were initially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef399ef",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "Further on with the `Mini-max algorithm` enhancement, it is possible to make the algorithm itself more advanced. For that I used two of the provided optimization techniques: `Progressive Deepening` and `Transposition Tables`.\n",
    "\n",
    "`Progressive deepening` or `Iterative deepening search` is a state space/graph search strategy in which a depth-limited version of depth-first search is run repeatedly with increasing depth limits until the goal is found. The algorithm is optimal, meaning that it finds the shallowest goal. Since it visits all the nodes in the search tree down to depth `d` before visiting any nodes at depth `d + 1`, the cumulative order in which nodes are first visited is effectively the same as in breadth-first search. However, `Iterative deepening search` uses much less memory.\n",
    "\n",
    "In my implementation the algorithm uses a range of depths to find the best score for the upcoming move and chooses the highest one obtained.\n",
    "\n",
    "`Transposition tables` (inititally used in chess) let us optimize calculating the best move when we encounter situations where different plays results being in the same end state. Essentially once we get to one specific Pacman position we just store the result of the minimax calculation at that position in the transposition table. This means that later on if some other different list of moves arrives at the same position then it is not necessary to completely recalculate the minimax at that position because the operation was already done before and we can just look it up from the transposition table.\n",
    "\n",
    "The implementation of these enhancements is rather straightforward and is shown in the code snippet below. After running the game with these changes the Pacman started making decisions quicker than in the default Mini-max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c421ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run this cell\n",
    "\n",
    "class MinimaxAgent(MultiAgentSearchAgent):\n",
    "\n",
    "    def __init__(self, evalFn='scoreEvaluationFunction', depth='2'):\n",
    "        super().__init__(evalFn, depth)\n",
    "        self.transpositionTable = {}\n",
    "\n",
    "    def getAction(self, gameState):\n",
    "        bestAction = None\n",
    "        bestScore = float('-inf')\n",
    "\n",
    "        for currentDepth in range(1, self.depth + 1):\n",
    "            self.transpositionTable.clear()\n",
    "\n",
    "            for action in gameState.getLegalActions(0):\n",
    "                score = self.minimax(\n",
    "                    1, currentDepth, gameState.generateSuccessor(0, action))\n",
    "\n",
    "                if score > bestScore:\n",
    "                    bestScore = score\n",
    "                    bestAction = action\n",
    "\n",
    "        return bestAction\n",
    "\n",
    "    def minimax(self, agentIndex, depth, gameState):\n",
    "        stateKey = (agentIndex, depth, gameState)\n",
    "\n",
    "        if stateKey in self.transpositionTable:\n",
    "            return self.transpositionTable[stateKey]\n",
    "\n",
    "        if depth == 0 or gameState.isWin() or gameState.isLose():\n",
    "            score = self.evaluationFunction(gameState)\n",
    "            self.transpositionTable[stateKey] = score\n",
    "            return score\n",
    "\n",
    "        numAgents = gameState.getNumAgents()\n",
    "        nextAgent = (agentIndex + 1) % numAgents\n",
    "        nextDepth = depth - 1 if nextAgent == 0 else depth\n",
    "\n",
    "        legalActions = gameState.getLegalActions(agentIndex)\n",
    "\n",
    "        if agentIndex == 0:\n",
    "            score = max(self.minimax(nextAgent, nextDepth, gameState.generateSuccessor(\n",
    "                agentIndex, action)) for action in legalActions)\n",
    "        else:\n",
    "            score = min(self.minimax(nextAgent, nextDepth, gameState.generateSuccessor(\n",
    "                agentIndex, action)) for action in legalActions)\n",
    "\n",
    "        self.transpositionTable[stateKey] = score\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf5e83f",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "\n",
    "Another enhancement which can be made to the `Mini-max algorithm` is changing the path finding algorithm to the `A* search algorithm`. `A* Search` is an informed best-first search algorithm that efficiently determines the lowest cost path between any two nodes in a directed weighted graph with non-negative edge weights. Its evaluation function is used to determine which node to explore next. The evaluation function, `f(x)`, for the `A* search algorithm` is the following: `f(x) = g(x) + h(x)` where `g(x)` represents the cost to get to node `x` and `h(x)` represents the estimated cost to arrive at the goal node from node `x`. For the algorithm to generate the correct result, the evaluation function must be `admissible`, meaning that it never overestimates the cost to arrive at the goal node.\n",
    "\n",
    "`h(x)` or `heuristic function` in my implementation returns the minimal distance to a pellet (from the list of pellets of the current game state) using `manhattan distance` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19fcea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run this cell\n",
    "\n",
    "def heuristic(gameState):\n",
    "    pacmanPos = gameState.getPacmanPosition()\n",
    "    food = gameState.getFood()\n",
    "    minPelletDistance = float('inf')\n",
    "\n",
    "    for x in range(food.width):\n",
    "        for y in range(food.height):\n",
    "            if food[x][y]:\n",
    "                distance = manhattanDistance(pacmanPos, (x, y))\n",
    "\n",
    "                if distance < minPelletDistance:\n",
    "                    minPelletDistance = distance\n",
    "\n",
    "    return minPelletDistance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb035f8",
   "metadata": {},
   "source": [
    "The complete `A* Mini-max algorithm` is presented in the cell below. It is worth mentioning that the search optimization was applied to the default `Mini-max` that is why in terms of performance it is slower than the `Mini-max` with enhancements added in the previous task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run this cell\n",
    "\n",
    "class AStarMinimaxAgent(MultiAgentSearchAgent):\n",
    "\n",
    "    def getAction(self, gameState):\n",
    "        bestAction = None\n",
    "        bestScore = float('-inf')\n",
    "\n",
    "        for action in gameState.getLegalActions(0):\n",
    "            score = self.aStarMinimax(\n",
    "                1, self.depth, gameState.generateSuccessor(0, action))\n",
    "\n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestAction = action\n",
    "\n",
    "        return bestAction\n",
    "\n",
    "    def aStarMinimax(self, agentIndex, depth, gameState):\n",
    "        pq = PriorityQueue()\n",
    "        pq.push((agentIndex, depth, gameState, 0), 0)\n",
    "\n",
    "        while not pq.isEmpty():\n",
    "            agentIndex, depth, gameState, cost = pq.pop()\n",
    "\n",
    "            if depth == 0 or gameState.isWin() or gameState.isLose():\n",
    "                return self.evaluationFunction(gameState) - heuristic(gameState)\n",
    "\n",
    "            numAgents = gameState.getNumAgents()\n",
    "            nextAgent = (agentIndex + 1) % numAgents\n",
    "            nextDepth = depth - 1 if nextAgent == 0 else depth\n",
    "\n",
    "            legalActions = gameState.getLegalActions(agentIndex)\n",
    "\n",
    "            if agentIndex == 0:\n",
    "                score = float('-inf')\n",
    "                for action in legalActions:\n",
    "                    successor = gameState.generateSuccessor(agentIndex, action)\n",
    "                    newCost = cost + \\\n",
    "                        self.evaluationFunction(\n",
    "                            successor) - heuristic(successor)\n",
    "\n",
    "                    pq.push((nextAgent, nextDepth, successor, newCost), newCost)\n",
    "                    score = max(score, newCost)\n",
    "            else:\n",
    "                score = float('inf')\n",
    "                for action in legalActions:\n",
    "                    successor = gameState.generateSuccessor(agentIndex, action)\n",
    "                    newCost = cost + \\\n",
    "                        self.evaluationFunction(\n",
    "                            successor) - heuristic(successor)\n",
    "\n",
    "                    pq.push((nextAgent, nextDepth, successor, newCost), newCost)\n",
    "                    score = min(score, newCost)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786cce90",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "\n",
    "In order to incorporate `A* search algorithm` into the `Alpha-beta pruning algorithm` it is only neccesary to introduce the `priority queue` to store the nodes which have to be explored for both variants - when the max agent (Pacman) is searching for an optimal move and when min agent (Ghost) performs the opposite action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa14eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run this cell\n",
    "\n",
    "class AStarAlphaBetaAgent(MultiAgentSearchAgent):\n",
    "\n",
    "    def getAction(self, gameState):\n",
    "        bestAction = None\n",
    "        bestScore = float('-inf')\n",
    "        alpha = float('-inf')\n",
    "        beta = float('inf')\n",
    "\n",
    "        pq = PriorityQueue()\n",
    "        for action in gameState.getLegalActions(0):\n",
    "            pq.push((action, 1, self.depth, gameState.generateSuccessor(\n",
    "                0, action), alpha, beta), 0)\n",
    "\n",
    "        while not pq.isEmpty():\n",
    "            action, agentIndex, depth, state, alpha, beta = pq.pop()\n",
    "            score = self.aStarAlphaBeta(agentIndex, depth, state, alpha, beta)\n",
    "\n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestAction = action\n",
    "\n",
    "        return bestAction\n",
    "\n",
    "    def aStarAlphaBeta(self, agentIndex, depth, gameState, alpha, beta):\n",
    "        if depth == 0 or gameState.isWin() or gameState.isLose():\n",
    "            return self.evaluationFunction(gameState) - heuristic(gameState)\n",
    "\n",
    "        numAgents = gameState.getNumAgents()\n",
    "        nextAgent = (agentIndex + 1) % numAgents\n",
    "        nextDepth = depth - 1 if nextAgent == 0 else depth\n",
    "\n",
    "        legalActions = gameState.getLegalActions(agentIndex)\n",
    "\n",
    "        if agentIndex == 0:  # Pacman's turn to maximize\n",
    "            value = float('-inf')\n",
    "            pq = PriorityQueue()\n",
    "\n",
    "            for action in legalActions:\n",
    "                successor = gameState.generateSuccessor(agentIndex, action)\n",
    "\n",
    "                newCost = self.evaluationFunction(\n",
    "                    successor) - heuristic(successor)\n",
    "                pq.push((action, nextAgent, nextDepth,\n",
    "                        successor, alpha, beta), newCost)\n",
    "\n",
    "            while not pq.isEmpty():\n",
    "                _, nextAgent, nextDepth, successor, alpha, beta = pq.pop()\n",
    "                value = max(value, self.aStarAlphaBeta(\n",
    "                    nextAgent, nextDepth, successor, alpha, beta))\n",
    "\n",
    "                alpha = max(alpha, value)\n",
    "\n",
    "                if alpha >= beta:\n",
    "                    break\n",
    "\n",
    "            return value\n",
    "        else:\n",
    "            value = float('inf')\n",
    "            pq = PriorityQueue()\n",
    "\n",
    "            for action in legalActions:\n",
    "                successor = gameState.generateSuccessor(agentIndex, action)\n",
    "\n",
    "                newCost = self.evaluationFunction(\n",
    "                    successor) - heuristic(successor)\n",
    "                pq.push((action, nextAgent, nextDepth,\n",
    "                        successor, alpha, beta), newCost)\n",
    "\n",
    "            while not pq.isEmpty():\n",
    "                _, nextAgent, nextDepth, successor, alpha, beta = pq.pop()\n",
    "                value = min(value, self.aStarAlphaBeta(\n",
    "                    nextAgent, nextDepth, successor, alpha, beta))\n",
    "\n",
    "                beta = min(beta, value)\n",
    "\n",
    "                if alpha >= beta:\n",
    "                    break\n",
    "\n",
    "            return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e29d59a",
   "metadata": {
    "id": "0e29d59a"
   },
   "source": [
    "## Conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7m6C-NhzzLy8",
   "metadata": {
    "id": "7m6C-NhzzLy8"
   },
   "source": [
    "In this laboratory work I primarily investigated Mini-max adversarial search algorithm and its modifications. By applying various enhancements I observed the changes in Pacman behavior during the game. I found out that Alpha-Beta pruning significantly increases Pacman's speed of making decisions. Progressive Deepening and Transposition Tables techniques led to the same result when being tested. At the same time by introducing additional parameters in the evaluation function I avoided situations when the Pacman got stuck in one position, however it still was not able to collect all the pellets to complete the game. Overall, adversarial search algorithms from this laboratory work are still not ideal, but they perform at a decent level in gaming problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e08029",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "In this laboratory work I was assisted by Arteom Kalamaghin from FAF-211. During our discussion he explained the use of A* search algorithm in the Mini-max algorithm implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zwGzGeqmzU-l",
   "metadata": {
    "id": "zwGzGeqmzU-l"
   },
   "source": [
    "## Bibliography:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82211848",
   "metadata": {},
   "source": [
    "1. https://www.javatpoint.com/mini-max-algorithm-in-ai\n",
    "2. https://medium.com/@aaronbrennan.brennan/minimax-algorithm-and-alpha-beta-pruning-646beb01566c\n",
    "3. https://en.wikipedia.org/wiki/Iterative_deepening_depth-first_search\n",
    "4. https://stackoverflow.com/questions/20009796/transposition-tables\n",
    "5. https://www.codecademy.com/resources/docs/ai/search-algorithms/a-star-search"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
